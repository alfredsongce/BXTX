# ðŸš€ ç§»åŠ¨èŒƒå›´æ˜¾ç¤ºç³»ç»Ÿ - ç¼“å­˜ç»„ä»¶ï¼ˆå¢žå¼ºç‰ˆï¼‰
extends Node
class_name MoveRangeCache

# ðŸš€ ç¼“å­˜å­˜å‚¨ï¼ˆæ·±åº¦æ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰
var _global_cache: Dictionary = {}
var _predictive_cache: Dictionary = {}
var _quick_cache: Dictionary = {}  # ðŸš€ æ–°å¢žï¼šå¿«é€Ÿé¢„è§ˆç¼“å­˜
var _cache_lookup_table: Dictionary = {}  # ðŸš€ æ–°å¢žï¼šå¿«é€ŸæŸ¥æ‰¾è¡¨
var _performance_stats: Dictionary = {
	"cache_hits": 0,
	"cache_misses": 0,
	"avg_compute_time": 0.0,
	"memory_usage_mb": 0.0,
	"gpu_compute_count": 0,
	"predictive_hits": 0,
	"batch_processed": 0,
	"quick_cache_hits": 0  # ðŸš€ æ–°å¢ž
}

# ðŸš€ æ–°å¢žï¼šç§»åŠ¨åŽ†å²è®°å½•ç”¨äºŽé¢„æµ‹
var _movement_history: Array = []
var _batch_queue: Array = []
var _preload_queue: Array = []  # ðŸš€ æ–°å¢žï¼šé¢„åŠ è½½é˜Ÿåˆ—

# ðŸ“¡ ä¿¡å·
signal cache_hit(key: String, cache_type: String)
signal cache_miss(key: String)
signal memory_warning(current_mb: float, limit_mb: float)
signal cache_cleaned(removed_count: int)
signal preload_completed(character_name: String)  # ðŸš€ æ–°å¢ž

# ðŸ”§ ç»„ä»¶å¼•ç”¨
var config  # æ”¹ä¸ºåŠ¨æ€ç±»åž‹

func _ready():
	print("ðŸš€ [Cache] ç¼“å­˜ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
	
	# èŽ·å–é…ç½®ç»„ä»¶å¼•ç”¨
	call_deferred("_setup_config_reference")
	
	# å¯åŠ¨å®šæ—¶æ¸…ç†
	_setup_cleanup_timer()
	
	# å¯åŠ¨é¢„æµ‹ç¼“å­˜å®šæ—¶å™¨
	_setup_predictive_timer()
	
	# ðŸš€ æ–°å¢žï¼šå¯åŠ¨é¢„åŠ è½½å¤„ç†å™¨
	_setup_preload_processor()

func _setup_config_reference():
	config = get_node("../Config")
	if not config:
		push_warning("[Cache] æœªæ‰¾åˆ°Configç»„ä»¶")

func _setup_cleanup_timer():
	var cleanup_timer = Timer.new()
	cleanup_timer.wait_time = 30.0
	cleanup_timer.timeout.connect(_cleanup_expired_cache)
	cleanup_timer.autostart = true
	add_child(cleanup_timer)

func _setup_predictive_timer():
	var predict_timer = Timer.new()
	predict_timer.wait_time = 2.0
	predict_timer.timeout.connect(_update_predictive_cache)
	predict_timer.autostart = true
	add_child(predict_timer)

func _setup_preload_processor():
	var preload_timer = Timer.new()
	preload_timer.wait_time = 0.1  # 100mså¤„ç†ä¸€æ¬¡é¢„åŠ è½½é˜Ÿåˆ—
	preload_timer.timeout.connect(_process_preload_queue)
	preload_timer.autostart = true
	add_child(preload_timer)

# ðŸŽ¯ å¢žå¼ºçš„ç¼“å­˜æŸ¥è¯¢æŽ¥å£ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰
func get_cached_texture(key: String) -> ImageTexture:
	var start_time = Time.get_ticks_usec()
	
	# ðŸš€ ä¼˜åŒ–1: ä½¿ç”¨æŸ¥æ‰¾è¡¨å¿«é€Ÿå®šä½
	if _cache_lookup_table.has(key):
		var cache_type = _cache_lookup_table[key]
		var texture = null
		
		match cache_type:
			"quick":
				texture = _quick_cache[key].texture
				_performance_stats.quick_cache_hits += 1
				cache_hit.emit(key, "quick")
			"predictive":
				texture = _predictive_cache[key].texture
				_performance_stats.predictive_hits += 1
				cache_hit.emit(key, "predictive")
			"global":
				texture = _global_cache[key].texture
				_performance_stats.cache_hits += 1
				cache_hit.emit(key, "global")
		
		if texture:
			# æ›´æ–°æœ€åŽä½¿ç”¨æ—¶é—´
			_update_last_used_time(key, cache_type)
			
			var lookup_time = (Time.get_ticks_usec() - start_time) / 1000.0
			if config and config.debug_mode >= 4:
				print("âš¡ [Cache] å¿«é€Ÿå‘½ä¸­ %s (%.1fms)" % [cache_type, lookup_time])
			
			return texture
	
	# ç¼“å­˜æœªå‘½ä¸­
	_performance_stats.cache_misses += 1
	cache_miss.emit(key)
	return null

# ðŸš€ æ–°å¢žï¼šæ›´æ–°æœ€åŽä½¿ç”¨æ—¶é—´ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
func _update_last_used_time(key: String, cache_type: String):
	var current_time = Time.get_ticks_msec()
	match cache_type:
		"quick":
			if _quick_cache.has(key):
				_quick_cache[key].last_used = current_time
		"predictive":
			if _predictive_cache.has(key):
				_predictive_cache[key].last_used = current_time
		"global":
			if _global_cache.has(key):
				_global_cache[key].last_used = current_time

# ðŸŽ¯ å¢žå¼ºçš„ç¼“å­˜å­˜å‚¨æŽ¥å£ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆï¼‰
func store_texture(key: String, texture: ImageTexture, cache_type: String = "global"):
	if not texture:
		push_warning("[Cache] å°è¯•å­˜å‚¨ç©ºçº¹ç†")
		return
	
	var cache_entry = {
		"texture": texture,
		"last_used": Time.get_ticks_msec(),
		"size_mb": _estimate_texture_size_mb(texture),
		"computation_time": 0.0
	}
	
	# ðŸš€ ä¼˜åŒ–ï¼šåŒæ—¶æ›´æ–°ç¼“å­˜å’ŒæŸ¥æ‰¾è¡¨
	match cache_type:
		"global":
			_global_cache[key] = cache_entry
		"predictive":
			_predictive_cache[key] = cache_entry
		"quick":
			_quick_cache[key] = cache_entry
		_:
			push_warning("[Cache] æœªçŸ¥çš„ç¼“å­˜ç±»åž‹: " + cache_type)
			return
	
	# æ›´æ–°æŸ¥æ‰¾è¡¨
	_cache_lookup_table[key] = cache_type
	
	_update_memory_usage()
	_check_memory_limit()

# ðŸš€ æ–°å¢žï¼šé¢„åŠ è½½çº¹ç†ï¼ˆåŽå°å¤„ç†ï¼‰
func preload_texture_for_character(character_name: String, qinggong_skill: int, position: Vector2):
	var preload_item = {
		"character_name": character_name,
		"qinggong_skill": qinggong_skill,
		"position": position,
		"timestamp": Time.get_ticks_msec(),
		"priority": 1  # 1=é«˜ä¼˜å…ˆçº§ï¼Œ2=ä¸­ç­‰ï¼Œ3=ä½Žä¼˜å…ˆçº§
	}
	
	_preload_queue.append(preload_item)
	
	if config and config.debug_mode >= 3:
		print("ðŸ“¥ [Cache] æ·»åŠ é¢„åŠ è½½ä»»åŠ¡: %s" % character_name)

# ðŸš€ æ–°å¢žï¼šå¤„ç†é¢„åŠ è½½é˜Ÿåˆ—
func _process_preload_queue():
	if _preload_queue.is_empty():
		return
	
	# æ¯æ¬¡åªå¤„ç†ä¸€ä¸ªï¼Œé¿å…é˜»å¡ž
	var item = _preload_queue.pop_front()
	_execute_preload_item(item)

func _execute_preload_item(item: Dictionary):
	var char_name = item.character_name
	var qinggong = item.qinggong_skill
	var position = item.position
	
	# ç”Ÿæˆé¢„åŠ è½½çš„ç¼“å­˜key
	var preload_key = "preload_%s_%d_%.0f_%.0f" % [char_name, qinggong, position.x, position.y]
	
	# æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
	if _cache_lookup_table.has(preload_key):
		return
	
	# åˆ›å»ºç®€åŒ–çš„é¢„è§ˆçº¹ç†
	var preview_texture = _create_simple_preview_texture(qinggong)
	if preview_texture:
		store_texture(preload_key, preview_texture, "quick")
		preload_completed.emit(char_name)
		
		if config and config.debug_mode >= 3:
			print("âœ… [Cache] é¢„åŠ è½½å®Œæˆ: %s" % char_name)

# ðŸš€ æ–°å¢žï¼šåˆ›å»ºç®€åŒ–é¢„è§ˆçº¹ç†
func _create_simple_preview_texture(qinggong_skill: int) -> ImageTexture:
	var resolution = 64  # å›ºå®šä½Žåˆ†è¾¨çŽ‡
	var image = Image.create(resolution, resolution, false, Image.FORMAT_RGBA8)
	var center = Vector2(int(resolution / 2), int(resolution / 2))  # ä½¿ç”¨int()å‡½æ•°è¿›è¡Œæ•´æ•°é™¤æ³•
	var scale = float(qinggong_skill * 2) / resolution
	
	# å¿«é€Ÿåœ†å½¢å¡«å……
	for x in range(resolution):
		for y in range(resolution):
			var distance = Vector2(x, y).distance_to(center) * scale
			if distance <= qinggong_skill:
				var alpha = 0.4 * (1.0 - distance / qinggong_skill)
				image.set_pixel(x, y, Color(0.0, 0.8, 0.0, alpha))
	
	var texture = ImageTexture.new()
	texture.set_image(image)
	return texture

# ðŸš€ æ™ºèƒ½é¢„æµ‹æ€§ç¼“å­˜æ›´æ–°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
func _update_predictive_cache():
	if not config or not config.enable_predictive_cache:
		return
	
	if _movement_history.size() < 2:  # é™ä½Žè¦æ±‚ï¼Œæé«˜å“åº”æ€§
		return
	
	# ðŸš€ å¼‚æ­¥å¤„ç†ï¼Œé¿å…é˜»å¡ž
	call_deferred("_predict_and_preload")

func _predict_and_preload():
	var recent_movements = _movement_history.slice(-2)  # åªç”¨æœ€è¿‘2æ¬¡
	var predicted_positions = _predict_next_positions_fast(recent_movements)
	
	# æ·»åŠ åˆ°é¢„åŠ è½½é˜Ÿåˆ—è€Œä¸æ˜¯æ‰¹é‡é˜Ÿåˆ—
	for prediction in predicted_positions:
		var char_name = prediction.get("character_name", "unknown")
		preload_texture_for_character(char_name, prediction.range, prediction.position)

# ðŸš€ å¿«é€Ÿä½ç½®é¢„æµ‹ï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰
func _predict_next_positions_fast(movements: Array) -> Array:
	var predictions = []
	
	if movements.size() < 2:
		return predictions
	
	var last_move = movements[-1]
	var prev_move = movements[-2]
	
	# ç®€åŒ–é¢„æµ‹ï¼šåªé¢„æµ‹ä¸€ä¸ªæœ€å¯èƒ½çš„ä½ç½®
	var direction = last_move.position - prev_move.position
	var predicted_pos = last_move.position + direction
	
	predictions.append({
		"position": predicted_pos,
		"range": last_move.range,
		"character_name": last_move.get("character_name", "unknown"),
		"weight": 1.0
	})
	
	return predictions

# ðŸš€ æ–°å¢žï¼šè®°å½•ç§»åŠ¨åŽ†å²
func record_movement_history(position: Vector2, range_val: int):
	var record = {
		"position": position,
		"range": range_val,
		"timestamp": Time.get_ticks_msec()
	}
	
	_movement_history.append(record)
	
	# åªä¿ç•™æœ€è¿‘20æ¡è®°å½•
	if _movement_history.size() > 20:
		_movement_history = _movement_history.slice(-20)

# ðŸš€ æ–°å¢žï¼šæ‰¹é‡å¤„ç†
func add_to_batch_queue(character_id: String, position: Vector2, range_val: int):
	var batch_item = {
		"character_id": character_id,
		"position": position,
		"range": range_val,
		"timestamp": Time.get_ticks_msec()
	}
	_batch_queue.append(batch_item)

func process_batch_queue():
	if _batch_queue.is_empty():
		return
	
	var batch_size = min(3, _batch_queue.size())
	var current_batch = _batch_queue.slice(0, batch_size)
	_batch_queue = _batch_queue.slice(batch_size)
	
	for batch_item in current_batch:
		_execute_batch_computation(batch_item)
	
	_performance_stats.batch_processed += batch_size

func _execute_batch_computation(batch_item: Dictionary):
	var char_id = batch_item.get("character_id", "")
	var position = batch_item.get("position", Vector2.ZERO)
	var range_val = batch_item.get("range", 0)
	
	var cache_key = _generate_batch_cache_key(char_id, position, range_val)
	if not _global_cache.has(cache_key):
		# é€šçŸ¥Controlleréœ€è¦è®¡ç®—è¿™ä¸ªèŒƒå›´
		_request_background_computation(cache_key, position, range_val)

func _generate_batch_cache_key(char_id: String, position: Vector2, range_val: int) -> String:
	return "batch_%s_%.0f_%.0f_%d" % [char_id, position.x, position.y, range_val]

func _request_background_computation(cache_key: String, position: Vector2, range_val: int):
	# å‘é€ä¿¡å·è¯·æ±‚åŽå°è®¡ç®—
	# è¿™é‡Œå¯ä»¥è¿žæŽ¥åˆ°Controllerçš„åŽå°è®¡ç®—æ–¹æ³•
	pass

# ðŸš€ å¢žå¼ºçš„æ€§èƒ½ç»Ÿè®¡
func update_computation_stats(computation_time: float):
	if _performance_stats.avg_compute_time == 0.0:
		_performance_stats.avg_compute_time = computation_time
	else:
		_performance_stats.avg_compute_time = (_performance_stats.avg_compute_time + computation_time) / 2.0

func get_cache_efficiency() -> float:
	var total = _performance_stats.cache_hits + _performance_stats.cache_misses
	if total == 0:
		return 0.0
	return (_performance_stats.cache_hits * 100.0) / total

# ðŸ§¹ å¢žå¼ºçš„ç¼“å­˜ç®¡ç†
func clear_all_cache():
	var total_removed = _global_cache.size() + _predictive_cache.size() + _quick_cache.size()
	_global_cache.clear()
	_predictive_cache.clear()
	_quick_cache.clear()
	_movement_history.clear()
	_batch_queue.clear()
	_preload_queue.clear()
	_reset_performance_stats()
	cache_cleaned.emit(total_removed)
	print("ðŸ§¹ [Cache] æ¸…é™¤æ‰€æœ‰ç¼“å­˜: %d é¡¹" % total_removed)

# ðŸš€ æ–°å¢žï¼šæ¸…ç†åŒ…å«ç‰¹å®šä¿¡æ¯çš„ç¼“å­˜æ¡ç›®
func clear_cache_containing(search_pattern: String) -> int:
	var removed_count = 0
	var keys_to_remove = []
	
	# ðŸš€ ä¼˜åŒ–ï¼šæ‰¹é‡æ”¶é›†è¦åˆ é™¤çš„key
	for key in _cache_lookup_table.keys():
		if search_pattern in key:
			keys_to_remove.append(key)
	
	# ðŸš€ æ‰¹é‡åˆ é™¤ï¼Œå‡å°‘Dictionaryæ“ä½œæ¬¡æ•°
	for key in keys_to_remove:
		var cache_type = _cache_lookup_table[key]
		
		match cache_type:
			"global":
				_global_cache.erase(key)
			"predictive":
				_predictive_cache.erase(key)
			"quick":
				_quick_cache.erase(key)
		
		_cache_lookup_table.erase(key)
		removed_count += 1
	
	if removed_count > 0:
		call_deferred("_update_memory_usage")  # å¼‚æ­¥æ›´æ–°ï¼Œé¿å…é˜»å¡ž
		cache_cleaned.emit(removed_count)
		
		if config and config.debug_mode >= 2:
			print("ðŸ—‘ï¸ [Cache] å¿«é€Ÿæ¸…ç†åŒ¹é…ç¼“å­˜ '%s': %d é¡¹" % [search_pattern, removed_count])
	
	return removed_count

# ðŸš€ æ–°å¢žï¼šæ¸…ç†ä¸Žç‰¹å®šè§’è‰²ç›¸å…³çš„æ‰€æœ‰ç¼“å­˜
func clear_character_cache(character_name: String) -> int:
	var removed_count = 0
	
	# æ¸…ç†åŒ…å«è§’è‰²åç§°çš„ç¼“å­˜
	removed_count += clear_cache_containing(character_name)
	
	# æ¸…ç†ç§»åŠ¨åŽ†å²ä¸­è¯¥è§’è‰²çš„è®°å½•
	var old_history_size = _movement_history.size()
	_movement_history = _movement_history.filter(func(record): return not (character_name in str(record)))
	var history_removed = old_history_size - _movement_history.size()
	
	# æ¸…ç†æ‰¹é‡é˜Ÿåˆ—ä¸­è¯¥è§’è‰²çš„ä»»åŠ¡
	var old_queue_size = _batch_queue.size()
	_batch_queue = _batch_queue.filter(func(item): return item.get("character_id", "") != character_name)
	var queue_removed = old_queue_size - _batch_queue.size()
	
	if history_removed > 0 or queue_removed > 0:
		print("ðŸ—‘ï¸ [Cache] æ¸…ç†è§’è‰² '%s' ç›¸å…³æ•°æ®: åŽ†å²è®°å½• %d é¡¹, é˜Ÿåˆ—ä»»åŠ¡ %d é¡¹" % [character_name, history_removed, queue_removed])
	
	return removed_count + history_removed + queue_removed

# ðŸš€ æ–°å¢žï¼šæ¸…ç†åŒ…å«ä»»ä½•æŒ‡å®šä½ç½®çš„ç¼“å­˜
func clear_position_related_cache(positions: Array) -> int:
	var removed_count = 0
	
	for position in positions:
		var pos_str = "%.0f,%.0f" % [position.x, position.y]
		removed_count += clear_cache_containing(pos_str)
	
	return removed_count

func _cleanup_expired_cache():
	var current_time = Time.get_ticks_msec()
	var removed_count = 0
	
	# æ¸…ç†5åˆ†é’Ÿæœªä½¿ç”¨çš„å…¨å±€ç¼“å­˜
	var expired_keys = []
	for key in _global_cache.keys():
		if current_time - _global_cache[key].last_used > 300000:  # 5åˆ†é’Ÿ
			expired_keys.append(key)
	
	for key in expired_keys:
		_global_cache.erase(key)
		removed_count += 1
	
	# æ¸…ç†2åˆ†é’Ÿæœªä½¿ç”¨çš„é¢„æµ‹ç¼“å­˜
	expired_keys.clear()
	for key in _predictive_cache.keys():
		if current_time - _predictive_cache[key].last_used > 120000:  # 2åˆ†é’Ÿ
			expired_keys.append(key)
	
	for key in expired_keys:
		_predictive_cache.erase(key)
		removed_count += 1
	
	if removed_count > 0:
		_update_memory_usage()
		cache_cleaned.emit(removed_count)
		print("ðŸ§¹ [Cache] æ¸…ç†è¿‡æœŸç¼“å­˜: %d é¡¹" % removed_count)

# ðŸ“Š è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡æŽ¥å£
func get_performance_stats() -> Dictionary:
	return _performance_stats.duplicate()

func get_cache_hit_rate() -> float:
	var total = _performance_stats.cache_hits + _performance_stats.cache_misses + _performance_stats.quick_cache_hits
	return 0.0 if total == 0 else ((_performance_stats.cache_hits + _performance_stats.quick_cache_hits) * 100.0) / total

# ðŸ“Š è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡æŽ¥å£ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
func get_detailed_cache_info() -> Dictionary:
	return {
		"global_cache_size": _global_cache.size(),
		"predictive_cache_size": _predictive_cache.size(),
		"quick_cache_size": _quick_cache.size(),  # ðŸš€ æ–°å¢ž
		"lookup_table_size": _cache_lookup_table.size(),  # ðŸš€ æ–°å¢ž
		"memory_usage_mb": _performance_stats.memory_usage_mb,
		"hit_rate": get_cache_hit_rate(),
		"avg_compute_time": _performance_stats.avg_compute_time,
		"movement_history_size": _movement_history.size(),
		"batch_queue_size": _batch_queue.size(),
		"preload_queue_size": _preload_queue.size(),  # ðŸš€ æ–°å¢ž
		"gpu_compute_count": _performance_stats.gpu_compute_count,
		"predictive_hits": _performance_stats.predictive_hits,
		"quick_cache_hits": _performance_stats.quick_cache_hits  # ðŸš€ æ–°å¢ž
	}

# ðŸ”§ å†…éƒ¨è¾…åŠ©æ–¹æ³•
func _estimate_texture_size_mb(texture: ImageTexture) -> float:
	if not texture:
		return 0.0
	var image = texture.get_image()
	if not image:
		return 0.0
	return (image.get_width() * image.get_height() * 4) / (1024.0 * 1024.0)  # RGBA8

func _update_memory_usage():
	if _pending_cleanup:
		return
	
	_pending_cleanup = true
	call_deferred("_do_memory_update")

func _do_memory_update():
	var total_mb = 0.0
	
	# å¿«é€Ÿè®¡ç®—æ€»å†…å­˜ä½¿ç”¨
	for entry in _global_cache.values():
		total_mb += entry.size_mb
	for entry in _predictive_cache.values():
		total_mb += entry.size_mb
	for entry in _quick_cache.values():
		total_mb += entry.size_mb
	
	_performance_stats.memory_usage_mb = total_mb
	_pending_cleanup = false

func _check_memory_limit():
	if not config:
		return
	
	if _performance_stats.memory_usage_mb > config.memory_limit_mb:
		memory_warning.emit(_performance_stats.memory_usage_mb, config.memory_limit_mb)
		_force_cleanup_oldest()

func _force_cleanup_oldest():
	var all_entries = []
	
	for key in _global_cache.keys():
		all_entries.append({
			"key": key,
			"last_used": _global_cache[key].last_used,
			"cache_type": "global"
		})
	
	for key in _predictive_cache.keys():
		all_entries.append({
			"key": key,
			"last_used": _predictive_cache[key].last_used,
			"cache_type": "predictive"
		})
	
	for key in _quick_cache.keys():
		all_entries.append({
			"key": key,
			"last_used": _quick_cache[key].last_used,
			"cache_type": "quick"
		})
	
	all_entries.sort_custom(func(a, b): return a.last_used < b.last_used)
	
	var remove_count = int(all_entries.size() / 2)  # ä½¿ç”¨int()å‡½æ•°è¿›è¡Œæ•´æ•°é™¤æ³•
	var removed = 0
	
	for entry in all_entries:
		if removed >= remove_count:
			break
		
		if entry.cache_type == "global":
			_global_cache.erase(entry.key)
		elif entry.cache_type == "predictive":
			_predictive_cache.erase(entry.key)
		elif entry.cache_type == "quick":
			_quick_cache.erase(entry.key)
		removed += 1
	
	_update_memory_usage()
	cache_cleaned.emit(removed)
	print("ðŸ—‘ï¸ [Cache] å¼ºåˆ¶æ¸…ç†: %d é¡¹ï¼Œå½“å‰å†…å­˜: %.1fMB" % [removed, _performance_stats.memory_usage_mb])

func _reset_performance_stats():
	_performance_stats = {
		"cache_hits": 0,
		"cache_misses": 0,
		"avg_compute_time": 0.0,
		"memory_usage_mb": 0.0,
		"gpu_compute_count": 0,
		"predictive_hits": 0,
		"batch_processed": 0,
		"quick_cache_hits": 0
	} 

# ðŸš€ æ–°å¢žï¼šèŽ·å–å¿«é€Ÿç¼“å­˜ç»Ÿè®¡
func get_quick_cache_stats() -> Dictionary:
	return {
		"quick_cache_hits": _performance_stats.quick_cache_hits,
		"quick_cache_size": _quick_cache.size(),
		"preload_queue_size": _preload_queue.size()
	}

# ðŸš€ æ–°å¢žï¼šå†…å­˜ä¼˜åŒ–ï¼šå»¶è¿Ÿåžƒåœ¾å›žæ”¶
var _pending_cleanup: bool = false 
 
